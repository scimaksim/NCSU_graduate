{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SAS Config named: oda\n",
      "Pandas module not available. Setting results to HTML\n",
      "SAS Connection established. Subprocess id is 206386\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import saspy\n",
    "my_session = saspy.SASsession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical inference\n",
    "We have two methods to work with statistical inference: **estimation** and **hypothesis testing**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways in which we can estimate the value of a population parameter. \n",
    "1. **Point estimate**: a single number that is our best guess for the population parameter.\n",
    "\n",
    "Suppose we are wondering how much sleep new parents in Amsterdam lost after they had their first baby. Let's say that new parents are those who got a baby within the last six months. We draw a simple random sample of $n=60$ new parents and asked them how much hours per night they slept less than before they had a baby.\n",
    "\n",
    "Lets assume that the mean number of hours that the 60 respondents in the sample slept less after they had their first baby is 2.6 ($\\bar{x} = 2.6$). This means that a good point estimate for the mean number of lost sleeping hours in the population is 2.6. In other words, the statistic $\\bar{x}$, which in our case is 2.6 hours, is a good point estimate for the parameter $\\mu$.\n",
    "\n",
    "However, one individual point estimate does not tell us if we're close to the population parameter we're interested in. Therefore, we'll want to know the likely precision of the point estimate. This precision is computed using an interval estimate. \n",
    "\n",
    "2. **Interval estimate**: a range of values within which we expect the population parameter to fall.  \n",
    "\n",
    "On the basis of a sample mean of $\\bar{x}=2.6$ hours, we might predict that the mean lost sleeping hours of all new parents in Amsterdam lies somewhere between $\\mu = 2.3-2.9$. The probability that the interval contains the population value is called the **confidence level**. The confidence level always has a value close to 1; in most cases, is it 0.95, so we talk about a \"95% confidence interval\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence intervals for mean with **known** population standard deviation ($\\sigma$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we ask new parents how much sleep they lose per night after having a new baby. We find that the mean number of lost sleeping hours per night is $\\bar{x} = 2.6$ hours and the _sample_ standard deviation is $s=0.9$ hours. Suppose we also know the _population_ standard deviation, $\\sigma = 1.1$ hours. In practice, it is very unlikely that we'll know $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can construct a confidence interval based on the information from our sample ($\\bar{x}$, $s$) and the population standard deviation ($\\sigma$). As long as our sample is sufficiently large, the sampling distribution is normally distributed with \n",
    "\n",
    "$$\n",
    "\\mu_{\\bar{x}} = \\mu \\\\\n",
    "\\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}}\n",
    "$$\n",
    "\n",
    "For a 95% confidence, we can look up the z-score that corresponds to the probability of 0.025 and find that $z=\\pm1.96$.\n",
    "\n",
    "<center><img src=\"z_table.png\" style=\"width:400px\"/></center>\n",
    "\n",
    "\n",
    "Therefore, the 95% confidence interval is given by\n",
    "\n",
    "$$\n",
    "\\text{confidence interval} = \\bar{x} \\pm 1.96\\sigma_{\\bar{x}} \\\\\n",
    "\\text{where } \\sigma_{\\bar{x}} = \\frac{sigma}{\\sqrt{n}} \\\\\n",
    "------- \\\\\n",
    "\\sigma_{\\bar{x}} = \\frac{1.1}{\\sqrt{60}} \\approx 0.142 \\\\\n",
    "\\text{margin of error} = 1.96\\times0.142 \\approx 0.28 \\\\\\\n",
    "CI = 2.6 \\pm 0.28\n",
    "$$\n",
    "\n",
    "Therefore, we have 95% confidence that (2.32, 2.88) contains the actual population mean. If we were to draw an infinite number of samples with $n=60$ from our population, and if we computed the confidence interval for every sample with this margin of error, in 95% of the samples the population value will fall within the confidence interval. \n",
    "\n",
    "<center><img src=\"confidence_interval.png\" style=\"width:400px\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence intervals for mean with **un**known population standard deviation ($\\sigma$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we often do not know the population standard deviation, we need to estimate it. To do this, we often use the **t-distribution**.\n",
    "\n",
    "Suppose we ask $n=60$ new parents how much sleep they lose per night after having a new baby. We find that the mean number of lost sleeping hours per night is $\\bar{x} = 2.6$ hours and the _sample_ standard deviation is $s=0.9$ hours. \n",
    "\n",
    "To construct a 95% confidence interval, we estimate the sample standard deviation and use the formula\n",
    "\n",
    "$$\n",
    "\\bar{x} \\pm t_{95\\%}(se) \\\\\n",
    "\\text{where } se = \\frac{s}{\\sqrt{n}}\n",
    "$$\n",
    "\n",
    "where $se$ is the **standard error**. It is the estimated standard deviation of the sampling distribution of the sample mean. $s$ is the sample standard deviation. \n",
    "\n",
    "In our example,\n",
    "\n",
    "$$\n",
    "se = \\frac{0.9}{\\sqrt{60}} \\approx 0.116 \\\\\n",
    "CI = 2.6 \\pm (2.00)(0.116) \\\\\n",
    "= 2 \\pm 0.23 \\\\\n",
    "CI = (2.37, 2.83)\n",
    "$$\n",
    "\n",
    "To obtain a confidence interval for a population mean, two assumptions must be satisfied.\n",
    "1. Your data should be obtained by randomization.\n",
    "2. Your population should be approximately normally distributed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence interval for proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a large sample, the sampling distribution is normally distributed with a mean that is equal to the population proportion, $p$. \n",
    "\n",
    "$$\n",
    "\\hat{p} = p \\\\\n",
    "\\sigma_{proportion} = \\sqrt{\\frac{p(1-p)}{n}}\n",
    "$$\n",
    "\n",
    "The z-score that corresponds to the 95% confidence interval is 1.96. Therefore,   \n",
    "\n",
    "$$\n",
    "\\hat{p} \\pm z_{95\\%}\\sigma_{proportion} \\\\\n",
    "\\hat{p} \\pm 1.96\\sigma_{proportion} \\\\\n",
    "\\text{where } \\sigma_{proportion} = \\sqrt{\\frac{p(1-p)}{n}}\n",
    "$$\n",
    "\n",
    "Since we do not know the population parameter $p$, we can substitute an estimate: \n",
    "\n",
    "$$\n",
    "\\hat{p} \\pm z_{95\\%}(se) \\\\\n",
    "\\text{where } se = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n",
    "$$\n",
    "\n",
    "Our assumption is that $n\\hat{p} \\ge 10$ and $n(1-\\hat{p})\\ge10$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-step plan for confidence intervals\n",
    "1. Decide a confidence level\n",
    "2. Proportion or mean? Proportion = z-distribution, mean = t-distribution (requires computing degrees of freedom, $df = n-1$).\n",
    "3. Compute the endpoints of the confidence interval. \n",
    "4. Interpret the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often report an estimate and its **standard error (SE)**. Standard error is the standard deviation of a _statistic_ (rather than of the population). For a random sample, the standard error of the sample mean (SEM) is \n",
    "\n",
    "$$\n",
    "SE(\\bar{Y}) = \\frac{\\sigma}{\\sqrt{n}} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we do not know the standard deviation of the _population_ ($\\sigma$), we can estimate the standard error of the sample mean as\n",
    "\n",
    "$$\n",
    "\\hat{SE}(\\bar{Y}) = \\frac{s}{\\sqrt{n}} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are observing the wingspan of 25 butterflies with a sample mean of 3.53 and a sample standard deviation of 0.17, then\n",
    "\n",
    "$$\n",
    "SEM = \\frac{0.17}{\\sqrt{25}} \\approx 0.034 \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** What do we mean by the term \"estimation\"?\n",
    "\n",
    "**A.** Attempting to use data to give a value (or range of values) for a parameter.\n",
    "\n",
    "**Q.** Why do we need to report more than a point estimate when using data to describe a parameter?\n",
    "\n",
    "**A.** A point estimate alone does not given an idea of variability. We usually provide a standard error or a confidence interval as well.\n",
    "\n",
    "**Q.** What does the standard error of a statistic measure?\n",
    "\n",
    "**A.** Variability (standard deviation) in the statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequentist estimation via method of moments and maximum likelihood (the math behind estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A point _estimate_ (e.g. the actual number $\\bar{y}$ or $s$) is an observed value of a point _estimator_ (a random variable, such as $\\bar{Y}$ or $S$, that we will use to make an inference). $\\bar{Y}$, which we often observe in a normal distribution, is a common estimator of the population mean, $\\mu$. Estimators have several key properties:\n",
    "* If we take many different samples and look at the estimator across all of those samples, the average of the estimators should give the true population value (an \"unbiased\" estimator)\n",
    "* The estimator should have as little variation as possible from dataset to dataset (a small standard error, SE)\n",
    "* The bigger the sample, the closer the estimator is to the true population value (\"consistent\" estimator)\n",
    "\n",
    "A point estimator is a random quantity that has yet to be observed.  For instance, the idea of taking a sample mean of the amount of debt of 100 randomly selected people.  The quantity isn’t observed yet and is random – it has a distribution, mean, standard error, etc.  We usually denote estimators with uppercase values.\n",
    "\n",
    "A point estimate is a fixed quantity – the observed value of the estimator.  For instance, if we observed the 100 randomly selected people and their sample mean amount of debt was 175 thousand.  This quantity is fixed or known.  We usually denote estimates with lowercase values.\n",
    "\n",
    "**Q.** What do we mean by the term \"consistent estimator\"?\n",
    "\n",
    "**A.** An estimator that is observed arbitrarily close to the parameter as the sample size increases is a consistent estimator.  This just implies that our estimator eventually is observed right at the truth.\n",
    "\n",
    "Note: If the bias of the estimator is 0 or goes away as the sample size grows and the standard error shrinks toward 0 as the sample size grows, the estimator will be consistent! This should make some sense, bias tells us where the estimator is observed on average and standard error tells us how variable our estimator is. If on average we take on the true value and our variation decreases towards 0, we must be observing closer and closer to the truth!\n",
    "\n",
    "**Q.** What is the basic idea of the Method of Moments estimation procedure?\n",
    "\n",
    "**A.** We take sample moments (such as $\\bar{Y}$) and set them equal to population moments (such as E(Y)) and solve the equations of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two common methods for creating an estimator\n",
    "\n",
    "We hope our estimators have nice properties\n",
    "* Unbiased\n",
    "* Low variance/standard error\n",
    "* Consistency\n",
    "* Easy-to-use sampling distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method of Moments (MOM)**\n",
    "\n",
    "If we have a random sample, MOM uses the sample average and the population averages to create estimators. On the upside, MOM makes the finding of estimators easy and MOM estimators are consistent; on the downside, the distribution of the created estimator can be difficult to determine and gamma MOM estimators are not unbiased (taking more samples does not bring us closer to the right answer). \n",
    "\n",
    "_Gamma Distribution Example_: We have a gamma distribution $Y\\sim Gamma(\\alpha,\\beta)$ where $Y$ is the time spent reading a news article. For gamma distributions,\n",
    "\n",
    "$$\n",
    "\\mu = E(Y) = \\frac{\\alpha}{\\beta}\\\\\n",
    "\\sigma^2 = Var(Y) = \\frac{\\alpha}{\\beta^2} \\\\\n",
    "E(Y^2) = \\frac{\\alpha(\\alpha+1)}{\\beta^2}\n",
    "$$\n",
    "\n",
    "where $E(Y)$ and $E(Y^2)$ are the population (raw) _moments_. We can observe a random sample of times spent reading a news article (a random sample of $Y$s). The Law of Large Numbers states that, for large sample sizes, the sample averages should be very close to the population averages. That is,\n",
    "\n",
    "$$\n",
    "\\hat{\\alpha} = \\frac{\\bar{Y}^2}{\\frac{\\sum_{i=1}^{n} Y_{i}^2}{n} - \\bar{Y}^2} \\approx\\frac{\\bar{y}^2}{\\frac{\\sum_{i=1}^{n} y_{i}^2}{n} - \\bar{y}^2} \\\\ \\text{ and }\\\\\n",
    "\\hat{\\beta} = \\frac{\\bar{Y}}{\\frac{\\sum_{i=1}^{n} Y_{i}^2}{n} - \\bar{Y}^2} \\approx \\frac{\\bar{y}}{\\frac{\\sum_{i=1}^{n} y_{i}^2}{n} - \\bar{y}^2}\n",
    "$$\n",
    "\n",
    "_Binomial/Bernoulli MOM example_: Our population is all of the customers at a bank and each customer follows a Bernoulli distribution. Our parameter $p$ is the proportion of customers willing to open an additional account. We observe 40 random customers. We can define $X_{i}=1$ if a customer $i$ opens an account, where $X_{i} \\sim ^{iid} Ber(p)$. We can also define $Y$ as the number of customers opening an account, where \n",
    "\n",
    "$$\n",
    "Y = \\sum_{i=1}^{n} X_{i}\\sim Bin(n,p)\n",
    "$$\n",
    "\n",
    "What is the MOM estimator of $p$?\n",
    "\n",
    "MOM tells us to set the sample average to the population average.\n",
    "\n",
    "$$\n",
    "X \\sim Ber(p) \\\\\n",
    "E(X) = p \\\\\n",
    "Var(X) = p(1-p)\n",
    "$$\n",
    "\n",
    "Here, $\\bar{X} \\approx p$ and \n",
    "\n",
    "$$\n",
    "\\text{Sample proportion} = \\hat{p} = \\frac{Y}{n}\n",
    "$$\n",
    "\n",
    "This estimator is unbiased (on average, it gives us $p$) and consistent and\n",
    "\n",
    "$$\n",
    "\\text{Standard error} = SE(\\hat{p}) = \\sqrt{\\frac{p(1-p)}{n}}\n",
    "$$\n",
    "\n",
    "Moreover, $\\hat{p}$ can be approximated using a normal distribution so that\n",
    "\n",
    "$$\n",
    "\\hat{p} \\sim N(p,\\sqrt{\\frac{p(1-p)}{n}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Maximum Likelihood (ML)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML uses the assumed curve to find the \"most likely\" values of the parameters to produce the data we see. Mathematically, they are more difficult, but they are generally consistent and the distribution of an MLE can often be approximated with a normal curve.\n",
    "\n",
    "_Exponential distribution example_: The exponential distribution is a special case of the gamma with $\\alpha = 1$. All we need to estimate is the $\\beta$. If $Y$ is the time spent reading a news article, then \n",
    "\n",
    "$$\n",
    "Y \\sim Gamma(1, \\beta) \\sim (Exp \\beta) \\\\\n",
    "f(y) = \\beta e^{-\\beta y}, y \\gt 0 \\\\\n",
    "\\mu = E(Y) = \\frac{1}{\\beta} \\\\\n",
    "\\sigma^2 = Var(Y) = \\frac{1}{\\beta^2}\n",
    "$$\n",
    "\n",
    "The PDF tells us \"give me a $\\beta$ and I will give you an observed value $y$\". The likelihood reverses that and tells us \"give me an observed value $y$ and I will give you a $\\beta$\". \n",
    "\n",
    "$$\n",
    "f(y | \\beta) = \\beta e^{-\\beta y}, y \\gt 0, \\beta \\gt 0 \\\\\n",
    "L(\\beta | y) = \\beta e^{-\\beta y}, y \\gt 0, \\beta \\gt 0\n",
    "$$\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Tip:</b> For generic $y$ value, the MLE for the exponential distribution is \n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = \\frac{1}{y}\n",
    "$$\n",
    "\n",
    "That is, the most likely value of $\\beta$ to produce $y$ is 1/$y$. For a random sample of $Y$s, $\\hat{\\beta}_{MLE} = \\frac{1}{\\bar{Y}}$\n",
    "</div>\n",
    "\n",
    "For a random sample of $Y$s, \n",
    "\n",
    "$$\n",
    "Y_{i} \\sim ^{iid} Exp(\\beta), i=1,...,n\n",
    "$$\n",
    "\n",
    "We can find the joint distribution as follows\n",
    "\n",
    "$$\n",
    "A \\text{ independent } B = P(A \\cap B) = P(A)P(B)\\\\\n",
    "f(y_1, y_2,...,y_n) = f(y_1)f(y_2)...f(y_n) \\\\\n",
    "=\\beta e^{-\\beta y_1}\\beta e^{-\\beta y_2}...\\beta e^{-\\beta y_n} \\\\\n",
    "= \\beta^n e^{-\\beta \\sum_{i=1}^{n} y_{i}}\n",
    "$$\n",
    "\n",
    "**Q.** What is the difference between a joint distribution (say $f(y_1,...,y_n|parameters)$) and a likelihood (say $L(parameters|y_1,...,y_n)$)?\n",
    "\n",
    "**A.** With a joint distribution, we consider the parameters to be known and the y’s can be varied (for instance in finding $P(Y_{1} \\gt 10, Y_{2} \\gt 5)$. With a likelihood, we assume the data is known and that the parameters can be varied. This allows us to maximize the function with respect to the parameter value(s). The value of the parameter that corresponds to this maximum is called the maximum likelihood estimator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common ML and MOM comparison\n",
    "\n",
    "<center><img src=\"mom_ml_estimators.png\" style=\"width:600px\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central Limit Theorem and Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sampling distribution is the pattern and frequency of a statistic/estimator. For a random sample (iid) of size $n$ from a population with mean $\\mu$ and variance $\\sigma^2$, a good approximation to the distribution of the sample mean in a \"large\" sample is a normal distribution where\n",
    "\n",
    "$$\n",
    "\\bar{Y} \\sim N(\\mu, \\frac{\\sigma}{\\sqrt{n}}) \\\\\n",
    "\\text{with} \\\\\n",
    "Z = \\frac{\\bar{Y} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confidence interval is the range of values that contain the true parameter.\n",
    "* Often use 95% confidence\n",
    "  * For 100 samples, about 95 intervals would contain the population mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability statements about a Binomial can be approximated using either\n",
    "\n",
    "$$\n",
    "\\hat{p} \\sim N(p, \\sqrt{\\frac{p(1-p)}{n}}) \\\\\n",
    "\\text{with} \\\\\n",
    "Z = \\frac{\\hat{p} - p}{\\sqrt{\\frac{p(1-p)}{n}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or via\n",
    "\n",
    "$$\n",
    "Y \\sim N(np, \\sqrt{np(1-p)}) \\\\\n",
    "\\text{with} \\\\\n",
    "Z = \\frac{Y-np}{\\sqrt{np(1-p)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuity correction\n",
    "\n",
    "When is a continuity correction useful?\n",
    "\n",
    "Anytime we approximate the distribution of something discrete with a continuous distribution, we can utilize a continuity correction to improve our probability approximations. \n",
    "\n",
    "We saw the continuity correction applied to the Binomial and Poisson (both discrete distributions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confidence\n",
    "\n",
    "Confidence refers to how much we believe in our procedure. That is, if we are 95% confident, that means that our procedure will produce a confidence interval that capture the truth 95% of the time.\n",
    "\n",
    "Said another way, if we were to repeatedly sample people and find a confidence interval for the average amount of debt people have, 95% of the intervals we created would capture the truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.**: In a confidence interval of the form point estimate +/- MOE, the Margin of Error (MOE) is determined by which of the following?\n",
    "\n",
    "**A.**: (estimated) standard error of the estimator, level of confidence we want (a-$\\alpha$)100%, sampling distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%SAS my_session\n",
    "PROC FREQ Data=Color;\n",
    "  TABLES EYES/BINOMIAL(Wald);\n",
    "RUN;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common CIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For a single mean\n",
    "<center><img src=\"common_ci_single_mean.png\" style=\"width:800px\"/></center>\n",
    "\n",
    "Where $\\bar{Y}$ is the _sample_ average and $\\mu$ is the _population_ average. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A normal distribution is described by $\\mu$ and $\\sigma$. A t-distribution is described by degrees of freedom, calculated as sample size $n$ minus one ($t_{n-1}$, the last row in the chart above). \n",
    "\n",
    "**Q.**: Suppose we want to make inference for a population mean.  When should we use the “z” interval vs the “t” interval?  (i.e. when should we use the one-sample z interval instead of the one-sample t interval.)\n",
    "\n",
    "**A.**: The t interval should be used when your population is roughly normally distributed.  A z interval should be used when you have a ‘large’ sample size. \n",
    "Note: The t interval and z interval are basically the same for sample sizes above 40 or 50 or so!\n",
    "\n",
    "**Q.**: What is meant by paired data?  Why do we have to treat paired data differently than the previous “two-sample” case?\n",
    "\n",
    "**A.**: Data that consists of two measurements on the same (or very similar/matched) units is paired data.  Since the observations are made on very similar units we can’t assume the two observations are independent.\n",
    "\n",
    "**Q.**: How are the paired t-interval and the one-sample t-interval related?\n",
    "\n",
    "**A.**: The paired t-interval is equivalent to doing a one-sample t-interval on the differences of the paired data.  Both assume normality (either of the differences or of the single sample, respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-test in SAS\n",
    "Consider data on the length of court cases.\n",
    "* Population = all court cases in some district\n",
    "* Sample = 20 cases (variable days)\n",
    "* Make inference about the average length of all court cases, $\\mu$\n",
    "* Create a 99% confidence interval for $\\mu$ using ALPHA = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%SAS my_session\n",
    "\n",
    "PROC TTEST DATA = cases ALPHA = 0.01 PLOTS = all;\n",
    "  VAR days;\n",
    "RUN;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian vs. Frequentist perspectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous estimations and confidence intervals fall under the \"Frequentist\" viewpoint, which assumes a fixed situation and treats probability as a \"long-run relative frequency\", where $P(A) = \\frac{\\text{number of times A occurs}}{\\text{total number of repeated trials}}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian paradigm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combines prior belief with observed data to create an updated (posterior) belief. \n",
    "\n",
    "**Q.**: What is the biggest difference(s) between the Frequentist and Bayesian viewpoints?\n",
    "\n",
    "**A.**: \n",
    "* The Frequentist paradigm assumes the parameters are fixed and you have repeatable situations.  Our goal is usually to find a confidence interval, do a hypothesis test, or predict.\n",
    "\n",
    "* The Bayesian paradigm assumes the parameters are random and we quantify our belief through probability distributions.  Our goal is to find the posterior distribution and use that to inform decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes' Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that, given a condition B, our probability of A is \n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n",
    "$$\n",
    "\n",
    "According to the multiplication law,\n",
    "\n",
    "$$\n",
    "P(A \\cap B) = P(A|B)P(B) \\\\\n",
    "or \\\\\n",
    "P(A \\cap B) = P(B|A)P(A)\n",
    "$$ \n",
    "\n",
    "Bayes' Theorem simply replaces the numerator in the conditional probability.\n",
    "\n",
    "$$\n",
    "\\text{Bayes' Theorem: } P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.**: When we say events make up a partition, what does that mean?\n",
    "\n",
    "**A.**: The events are all disjoint and, in total, make up the entire space of events.\n",
    "\n",
    "**Q.**: What is meant by a prior distribution?\n",
    "\n",
    "**A.**: A distribution that specifies our prior belief about our parameters\n",
    "\n",
    "**Q.**: What is meant by our likelihood?\n",
    "\n",
    "**A.**: A distribution that models our data\n",
    "\n",
    "**Q.**: What is meant by a posterior distribution?\n",
    "\n",
    "**A.**: A distribution that contains our updated belief about our parameter(s) after seeing the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Set 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions:\n",
    "\n",
    "**1. What is the Frequentist viewpoint on statistics?**\n",
    "\n",
    "The Frequentist paradigm assumes the parameters are fixed and you have repeatable situations.  Our goal is usually to find a confidence interval, do a hypothesis test, or predict.\n",
    "\n",
    "Fixed parameters, situations are repeatable (static/fixed) situations. Problem is that things happen between now and the next week that may affect results. \n",
    "\n",
    "**2. What is the Bayesian viewpoint on statistics?**\n",
    "\n",
    "The Bayesian paradigm assumes the parameters _are random_ and we quantify our belief through probability distributions.  Our goal is to find the posterior distribution and use that to inform decisions.\n",
    "\n",
    "No true parameter, just a distribution that we look at. We put a prior distribution, model through likelihood, and then combine into a posterior distribution. \n",
    "\n",
    "**3. Should I be a Frequentist or a Bayesian?**\n",
    "\n",
    "Not really, choose what works best.\n",
    "\n",
    "**4. What are the terms prior, likelihood, and posterior?**\n",
    "\n",
    "Prior belief = some prior belief about our parameters. Whatever distribution we assume about our data before seeing that data.\n",
    "\n",
    "Likelihood = a model for our data. We need some experiment or a study to investigate $p$. Experiment may be to observe the number of successes $Y$ in $n$ iid trials. Likelihood is $Y|p ~ Bin(n,p)$. No fixed true value of $p$, just a distribution of $p$s. \n",
    "\n",
    "Posterior belief = an updated belief about our parameters after seeing the data. Combine prior and likelihood into a posterior distribution using Bayes' theorem. With a posterior, we're looking at a distribution and are trying to find a distribution for $p$ (a random thing) given our data. i.e. $p|Y=Y ~ ?$. $p$ still follows a Beta distribution (like our prior). $p|y ~ Beta(\\alpha +y, n-y+ \\beta)$.\n",
    "\n",
    "**5. What do we do once we have the posterior?**\n",
    "\n",
    "Summarize that posterior distribution in meaningful ways, with something like a posterior mean $E(p|y)$, a posterior median, and a posterior standard deviation. Give a credible interval (CI). \n",
    "\n",
    "**6. How are credible intervals and confidence intervals similar? different?**\n",
    "A credible interval includes 95% of our distribution. We can still observe it with a probability. \n",
    "\n",
    "In the Frequentist view, the probability is either a 0 or a 1.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Undergraduate students are classified into one of four groups:**\n",
    "* freshmen (35% of students)\n",
    "* sophomores (28% of students)\n",
    "* juniors (23% of students)\n",
    "* seniors (14% of students)\n",
    "\n",
    "**For the students that are freshmen 46% live on campus, for those that are sophomores 23% live on campus, for those that are juniors 17% live on campus, and for those that are seniors 14% live on campus.**\n",
    "\n",
    "**a. What is the probability a randomly selected student lives on campus?**\n",
    "\n",
    "**b. If a student lives on campus, what is the probability they are a senior?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. In a 2018 study, 1250 households across the U.S. were randomly selected for a survey the use of cellular versus landline phones. Of those surveyed, 715 responded that they do not have a landline phone and rely completely on their cellular phone. Suppose we want to make Bayesian inference on p = P(landline).**\n",
    "\n",
    "**a. What do we need to do first?**\n",
    "\n",
    "* Define our prior beliefs\n",
    "* Model the data appropriately\n",
    "* Combine info with Bayes' theorem to find posterior distribution\n",
    "\n",
    "**b. What is a reasonable likelihood?**\n",
    "$$\n",
    "Y = \\text{number of people with a landline} \\\\\n",
    "Y \\sim Bin(1250,p) \\\\\n",
    "Y|p \\sim Bin(1250,p)\n",
    "$$\n",
    "\n",
    "**c. What is our posterior distribution?**\n",
    "\n",
    "**d. Find and interpret a 95% credible interval for p.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Data was collected on 45 Americans and the number of hours of traditional TV watched in a week is recorded. The data is included in the TVData.xlsx file. Suppose we want to make Bayesian inference on the mean number of hours watched.**\n",
    "\n",
    "**a. What do we need to do first?**\n",
    "\n",
    "**b. What might we use as a reasonable likelihood?\n",
    "\n",
    "c. What is our posterior distribution?\n",
    "\n",
    "d. Report a posterior mean and find and interpret a 95% credible interval for the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01//EN\"\n",
       "   \"http://www.w3.org/TR/html4/strict.dtd\">\n",
       "<!--\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2021 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "-->\n",
       "<html>\n",
       "<head>\n",
       "  <title></title>\n",
       "  <meta http-equiv=\"content-type\" content=\"text/html; charset=None\">\n",
       "  <style type=\"text/css\">\n",
       "/*\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2021 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "*/\n",
       "pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "body .hll { background-color: #ffffcc }\n",
       "body { background: #ffffff; }\n",
       "body .c { color: #0000FF } /* Comment */\n",
       "body .k { color: #ff0000; font-weight: bold } /* Keyword */\n",
       "body .n { color: #008000 } /* Name */\n",
       "body .ch { color: #0000FF } /* Comment.Hashbang */\n",
       "body .cm { color: #0000FF } /* Comment.Multiline */\n",
       "body .cp { color: #0000FF } /* Comment.Preproc */\n",
       "body .cpf { color: #0000FF } /* Comment.PreprocFile */\n",
       "body .c1 { color: #0000FF } /* Comment.Single */\n",
       "body .cs { color: #0000FF } /* Comment.Special */\n",
       "body .kc { color: #ff0000; font-weight: bold } /* Keyword.Constant */\n",
       "body .kd { color: #ff0000; font-weight: bold } /* Keyword.Declaration */\n",
       "body .kn { color: #ff0000; font-weight: bold } /* Keyword.Namespace */\n",
       "body .kp { color: #ff0000; font-weight: bold } /* Keyword.Pseudo */\n",
       "body .kr { color: #ff0000; font-weight: bold } /* Keyword.Reserved */\n",
       "body .kt { color: #ff0000; font-weight: bold } /* Keyword.Type */\n",
       "body .s { color: #111111 } /* Literal.String */\n",
       "body .na { color: #008000 } /* Name.Attribute */\n",
       "body .nb { color: #008000 } /* Name.Builtin */\n",
       "body .nc { color: #008000 } /* Name.Class */\n",
       "body .no { color: #008000 } /* Name.Constant */\n",
       "body .nd { color: #008000 } /* Name.Decorator */\n",
       "body .ni { color: #008000 } /* Name.Entity */\n",
       "body .ne { color: #008000 } /* Name.Exception */\n",
       "body .nf { color: #008000 } /* Name.Function */\n",
       "body .nl { color: #008000 } /* Name.Label */\n",
       "body .nn { color: #008000 } /* Name.Namespace */\n",
       "body .nx { color: #008000 } /* Name.Other */\n",
       "body .py { color: #008000 } /* Name.Property */\n",
       "body .nt { color: #008000 } /* Name.Tag */\n",
       "body .nv { color: #008000 } /* Name.Variable */\n",
       "body .sa { color: #111111 } /* Literal.String.Affix */\n",
       "body .sb { color: #111111 } /* Literal.String.Backtick */\n",
       "body .sc { color: #111111 } /* Literal.String.Char */\n",
       "body .dl { color: #111111 } /* Literal.String.Delimiter */\n",
       "body .sd { color: #111111 } /* Literal.String.Doc */\n",
       "body .s2 { color: #111111 } /* Literal.String.Double */\n",
       "body .se { color: #111111 } /* Literal.String.Escape */\n",
       "body .sh { color: #111111 } /* Literal.String.Heredoc */\n",
       "body .si { color: #111111 } /* Literal.String.Interpol */\n",
       "body .sx { color: #111111 } /* Literal.String.Other */\n",
       "body .sr { color: #111111 } /* Literal.String.Regex */\n",
       "body .s1 { color: #111111 } /* Literal.String.Single */\n",
       "body .ss { color: #111111 } /* Literal.String.Symbol */\n",
       "body .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       "body .fm { color: #008000 } /* Name.Function.Magic */\n",
       "body .vc { color: #008000 } /* Name.Variable.Class */\n",
       "body .vg { color: #008000 } /* Name.Variable.Global */\n",
       "body .vi { color: #008000 } /* Name.Variable.Instance */\n",
       "body .vm { color: #008000 } /* Name.Variable.Magic */\n",
       "\n",
       "  </style>\n",
       "</head>\n",
       "<body>\n",
       "<h2></h2>\n",
       "\n",
       "<div class=\"highlight\"><pre><span></span><span class=\"s\">7                                                          The SAS System                        Tuesday, March 16, 2021 12:12:00 AM</span><br><br><span class=\"s\">34         ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode=&#39;inline&#39;) device=svg style=HTMLBlue;</span><br><span class=\"s\">34       ! ods graphics on / outputfmt=png;</span><br><span class=\"s\">35         </span><br><span class=\"s\">36         DATA geom;</span><br><span class=\"s\">37              INPUT trials;</span><br><span class=\"s\">38              DATALINES;</span><br><span class=\"s\">49         ;</span><br><span class=\"s\">50         </span><br><span class=\"s\">51         </span><br><span class=\"s\">52         </span><br><span class=\"s\">53         ods html5 (id=saspy_internal) close;ods listing;</span><br><span class=\"s\">54         </span><br><br><span class=\"s\">8                                                          The SAS System                        Tuesday, March 16, 2021 12:12:00 AM</span><br><br><span class=\"s\">55         </span><br></pre></div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%SAS my_session\n",
    "DATA geom;\n",
    "     INPUT trials;\n",
    "     DATALINES;\n",
    "5\n",
    "1\n",
    "3\n",
    "9\n",
    "6\n",
    "6\n",
    "5\n",
    "3\n",
    "8\n",
    "7\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%SAS my_session\n",
    "DATA pressure;\n",
    "   INPUT SBPbefore SBPafter;\n",
    "   DATALINES;\n",
    "120 128   \n",
    "124 131   \n",
    "130 131   \n",
    "118 127\n",
    "140 132   \n",
    "128 125   \n",
    "140 141   \n",
    "135 137\n",
    "126 118   \n",
    "130 132   \n",
    "126 129   \n",
    "127 135\n",
    ";\n",
    "\n",
    "PROC TTEST DATA = pressure PLOTS = all;\n",
    "PAIRED SBPbefore*SBPafter;\n",
    "RUN;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%SAS my_session\n",
    "PROC TTEST DATA = sashelp.bweight ALPHA = 0.1 PLOTS = all;\n",
    "VAR weight;\n",
    "RUN;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%SAS my_session\n",
    "PROC FREQ DATA=sashelp.bweight;\n",
    "TABLES Weight/BINOMIAL(Wald);\n",
    "RUN;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypotheses** are expectations about a _population_ (e.g. the parameters of that population).\n",
    "\n",
    "The **test statistic** is the number of standard errors that a sample value deviates by from the $H_{0}$ value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test about the _proportion_\n",
    "Support we assume that the number of Americans who have scuba-diving experience is less than 3%. Our null hypothesis is $H_{0}=0.03$ and our alternative hypothesis is $H_{A} \\lt 0.03$. We draw a sample of 1,000 Americans ($n=1000$) and find that the sample proportion is $p=0.02$. How likely is a sample proportion of 0.02 if the population proportion is 0.03?\n",
    "\n",
    "**Steps**:\n",
    "1. Compute the test statistic (or the number of standard error that the sample statistic is removed from the assumed population parameter)\n",
    "\n",
    "The number of standard errors thee sample statistic is removed from the assumed population parameter is represented by a **Z-score**.\n",
    "\n",
    "$$\n",
    "\\text{test statistic} = z = \\frac{\\hat{p}-p}{SE_{0}} \\\\\n",
    "\\text{where the standard error assumed under the null hypothesis} = SE_{0} = \\sqrt{\\frac{p(1-p)}{n}}\n",
    "$$\n",
    "\n",
    "$p$ is the population proportion assumed under the null hypothesis and $\\hat{p}$ is the sample proportion.\n",
    "\n",
    "Here,\n",
    "\n",
    "$$\n",
    "\\text{test statistic} = z = \\frac{0.02-0.03}{SE_{0}} \\\\\n",
    "\\text{where } SE_{0} = \\sqrt{\\frac{0.03(1-0.03)}{1000}} \\approx 0.005 \\\\\n",
    "z = \\frac{0.02-0.03}{0.005} \\approx -1.85\n",
    "$$\n",
    "\n",
    "This means that our _sample_ proportion falls 1.85 standard errors below the _population_ proportion when the null hypothesis ($H_{0}$) is true. The probability, or the **p-value**, that corresponds to this z-score is 0.0322 (3.22%), as seen in the z-table below.\n",
    "\n",
    "<center><img src=\"z_score_prob_value.png\" style=\"width:400px\"/></center>\n",
    "\n",
    "Thus, finding a sample proportion of 0.02 if the population proportion is actually 0.03 is unlikely. Is it _unlikely enough_ to reject the null hypothesis? It depends on the **significance level**, $\\alpha$, that we choose before conducting the experiment. A common $\\alpha$ value is 0.05. In this case, if the p-value is smaller than 0.05, we can say that \"our sample provides enough evidence to reject the null hypothesis\". Since $0.0322 \\lt 0.05$, we reject the null hypothesis. \n",
    "\n",
    "We can also state that our test statistic of -1.85 falls within the **rejection region**. The critical z-value that forms the border of the rejection region is -1.64, which can be found by looking up the left-tailed probability of 0.05.    \n",
    "\n",
    "<center><img src=\"rejection_region.png\" style=\"width:600px\"/></center>\n",
    "\n",
    "We thus reject our null hypothesis and conclude that the proportion of Americans with scuba-diving experience is lower than 0.03. \n",
    "\n",
    "Since our alternative hypothesis was that the population parameter is _smaller_ than 0.03, $H_{A}: p \\lt 0.03$, we only focused on one side of the sampling distribution (the left side). We performed a **one-tailed test**. If our alternative hypothesis had been $H_{A}: p \\neq 0.03$, we would not focuses just on the left side of the distribution but on both sides of the distribution. We would thus perform a **two-tailed test**. Based on the z-table, this would correspond to the critical values of -1.96 and 1.96.   \n",
    "\n",
    "<center><img src=\"two_tailed_test.png\" style=\"width:400px\"/></center>\n",
    "\n",
    "In this case, our test statistic of -1.85 does not fall in the rejection region. This means that we cannot reject the null hypothesis that $p=0.03$. \n",
    "\n",
    "Choosing a one- or a two-tailed test can make a big difference to your conclusions! In practice, two-tailed tests are used much more often. \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Tip:</b> Most significance tests are two-tailed and are based on a significance level ($\\alpha$) of 0.05! \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significance test about the _population mean_\n",
    "How long can scuba-divers stay under water? Suppose we expect that experienced American divers can stay under water for more than 60 minutes. Suppose we approach 100 America scuba-divers (n=100) and measure how long they can stay under water. We find that the mean time that these divers stay under water is 62 minutes ($\\bar{x} = 62 minutes$). The standard deviation is 5 minutes ($S=5 minutes$). \n",
    "\n",
    "Here, \n",
    "\n",
    "$$\n",
    "H_{0}: \\mu = 60 \\\\\n",
    "H_{A}: \\mu \\gt 60\n",
    "$$\n",
    "\n",
    "To conduct a significance test about the population mean, assess if it is likely that the sample we have collected actually comes from a population with a mean that equals the value in our null hypothesis.\n",
    "\n",
    "<center><img src=\"sample_distribution_sample_mean.png\" style=\"width:400px\"/></center>\n",
    "\n",
    "Steps:\n",
    "1. Compute a test statistic (the number of standard errors the sample mean is removed from the $H_{0}$ value according to the null hypothesis).\n",
    "\n",
    "\n",
    "**One-tailed test**\n",
    "\n",
    "To compute the standard error, we need to know the population standard deviation, $\\sigma$. Since we do not know $\\sigma$, we need to estimate it using the _sample_ standard deviation, $s$. Since this introduces extra error, we employ the **t-distribution**, rather than the z-distribution. \n",
    "\n",
    "$$\n",
    "\\text{Test statistic} = t = \\frac{\\bar{x}-\\mu_{0}}{SE} \\\\\n",
    "\\text{where standard error} = SE = \\frac{s}{\\sqrt{n}}\n",
    "$$\n",
    "\n",
    "Here,\n",
    "\n",
    "$$\n",
    "\\text{Standard error} = \\frac{5}{\\sqrt{100}} = 0.5 \\\\\n",
    "t = \\frac{62-60}{0.5} = 4\n",
    "$$\n",
    "\n",
    "Our test statistic is a t-score of 4. Is this enough to reject the null hypothesis? If we employ $\\alpha=0.05$ and do a one-tailed test, we can find the critical value (1.67) for our rejection region in a t table. We look at $t_{90\\%}$ because we want a cumulative probability of 0.05 in the right tail of the distribution.   \n",
    "\n",
    "<center><img src=\"t_test.png\" style=\"width:400px\"/></center>\n",
    "<center><img src=\"t_table_90.png\" style=\"width:400px\"/></center>\n",
    "\n",
    "Since $t=4$ falls into our rejection region, we can reject the null hypothesis and conclude that, on average, experienced American divers stay under water for more than 60 minutes.  \n",
    "\n",
    "<center><img src=\"sample_dist.png\" style=\"width:400px\"/></center>\n",
    "\n",
    "If, instead, we state that \n",
    "\n",
    "$$\n",
    "H_{0}: \\mu = 60 \\\\\n",
    "H_{A}: \\mu \\neq 60\n",
    "$$\n",
    "\n",
    "we have to do a two-tailed t-distribution test. \n",
    "\n",
    "**Two-tailed test**\n",
    "\n",
    "Suppose $\\alpha=0.01$. Our sampling distribution looks like this:\n",
    "\n",
    "<center><img src=\"two_tailed_sample_dist.png\" style=\"width:400px\"/></center>\n",
    "\n",
    "Our t-value is still 4 and is still in the rejection region. We still reject the null hypothesis and conclude that our finding is still statistically significant. With a two-tailed t-test, we conclude that the mean time that experienced American divers spend under water is NOT 60 minutes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-step plan for conducting significance tests\n",
    "Suppose you have two expectations:\n",
    "* More than half of all certified divers in America have more than 35 hours of diving experience. \n",
    "  * Here, we are dealing the a proportion, $p$.\n",
    "  \n",
    "* Mean number of hours of diving experience of all certified divers in America is more than 35 hours. \n",
    "  * Here, we are dealing with a mean, $\\mu$\n",
    "\n",
    "How many hours of experiences? In our sample of n=100, the distribution of the variables \"hours of diving experience\" is approximately normal.\n",
    "\n",
    "**Example 1**\n",
    "\n",
    "A proportion of 0.57 has more than 35 hours: $p(\\gt 35 \\text{ hours}) = 0.57$. Here, $H_{0}: p = 0.5$, $H_{A}: p \\gt 0.5$. We thus have to conduct right-tailed tests.\n",
    "\n",
    "In the case of proportions,\n",
    "$$\n",
    "z = \\frac{\\hat{p} - p}{SE_{0}} \\\\\n",
    "SE_{0} = \\sqrt{\\frac{p(1-p)}{n}}\n",
    "$$\n",
    "\n",
    "Here, \n",
    "$$\n",
    "SE_{0} = \\sqrt{\\frac{0.5(1-0.5)}{500}} \\\\\n",
    "z = \\frac{0.57 - 0.5}{\\sqrt{\\frac{0.5 \\times 0.5}{500}}} \\approx 3.13 \\\\\n",
    "$$\n",
    "\n",
    "**Example 2**\n",
    "\n",
    "Mean number of hours of diving experience is 35.5, the standard deviation is 8: $\\bar{x}=35.5$, $S=8$. Here, $H_{0}: \\mu = 35$, $H_{A}: \\mu \\gt 35$. We thus have to conduct right-tailed tests.\n",
    "\n",
    "In the case of the mean, \n",
    "$$\n",
    "SE = \\frac{8}{\\sqrt{500}} \\\\\n",
    "t = \\frac{35.5 - 35}{\\frac{8}{\\sqrt{500}}} \\approx 1.40 \\\\\n",
    "$$\n",
    "\n",
    "**Step-by-step plan**\n",
    "1. _Proportion or mean?_ In example 1, we're dealing with a proportion. In example 2, we're dealing with a mean. \n",
    "2. _Formulate your hypotheses_\n",
    "\n",
    "In the case of proportions, our \n",
    "$$\n",
    "H_{0}: p=p_{0} \\\\\n",
    "H_{A}: p \\neq p_{0} \\text{ or} \\\\\n",
    "H_{A}: p \\gt p_{0} \\text{ or} \\\\\n",
    "H_{A}: p \\lt p_{0}\n",
    "$$\n",
    "\n",
    "In the case of the mean, \n",
    "$$\n",
    "H_{0}: \\mu = \\mu_{0} \\\\\n",
    "H_{A}: \\mu \\neq \\mu_{0} \\text{ or} \\\\\n",
    "H_{A}: \\mu \\gt \\mu_{0} \\text{ or} \\\\\n",
    "H_{A}: \\mu \\lt \\mu_{0}\n",
    "$$\n",
    "3. _Check if your assumptions are met._\n",
    "\n",
    "In both cases 1 and 2, randomization is essential. Your data must have been collected by means of a random sample or a randomized experiment. \n",
    "\n",
    "In the case of proportions,\n",
    "$$\n",
    "np \\ge 10 \\text{ and} \\\\\n",
    "n(1-p) \\ge 10\n",
    "$$\n",
    "\n",
    "In the case of the mean, the population distribution should be approximately normal. \n",
    "\n",
    "4. Determine your significance level, $\\alpha$. Usually, $\\alpha=0.05$. \n",
    "5. Compute your test statistic.\n",
    "\n",
    "In the case of proportions,\n",
    "$$\n",
    "z = \\frac{\\hat{p} - p}{SE_{0}} \\\\\n",
    "SE_{0} = \\sqrt{\\frac{p(1-p)}{n}}\n",
    "$$\n",
    "\n",
    "In the case of the mean, \n",
    "$$\n",
    "t = \\frac{\\bar{x} - \\mu}{SE} \\\\\n",
    "SE = \\frac{S}{\\sqrt{n}}\n",
    "$$\n",
    "\n",
    "6. _Draw the sampling distribution_.\n",
    "<center><img src=\"step_6.png\" style=\"width:900px\"/></center>\n",
    "\n",
    "7. _Find location of test statistic_\n",
    "8. _Decide if the null hypothesis should be rejected_\n",
    "9. _Interpret your findings._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "A vet wants to determine the proportion of adult cats that have high tartar build-up on their teeth.  They are thinking of running a special on dental cleanings but need a large number of appointments to make it worth their while.\n",
    "\n",
    "They want to see if the proportion of adult cats with tartar build-up is greater than 0.7.\n",
    "\n",
    "Our null and alternative hypotheses are: \n",
    "\n",
    "$$\n",
    "H_0: p = 0.7, H_A: p >0.7\n",
    "$$\n",
    " \n",
    "\n",
    "We observe a random sample of 50 cats and find that 39 of them have tartar build-up.  Our test statistic is:\n",
    "\n",
    "$$\n",
    "Z = \\frac{\\hat{p}-0.7}{\\sqrt{0.7(1-0.7)/50}} \\sim^{H_0} N(0,1)\n",
    "$$\n",
    "\n",
    "What is the value of our p-value for testing these hypotheses? (Use two decimal places.)\n",
    "\n",
    "**Answer**\n",
    "\n",
    "$$\n",
    "P(Z \\ge z_{obs}) = P(Z \\ge 1.2344) = 1-P(Z \\le 1.2344)\n",
    "$$\n",
    "\n",
    "Using the standard normal CDF, this gives $1-0.8915=0.1085=0.11$. With a significance level of $\\alpha=0.05$, we fail to reject the null hypothesis that the proportion of adult cats with tartar buildup is 0.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01//EN\"\n",
       "   \"http://www.w3.org/TR/html4/strict.dtd\">\n",
       "<!--\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2021 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "-->\n",
       "<html>\n",
       "<head>\n",
       "  <title></title>\n",
       "  <meta http-equiv=\"content-type\" content=\"text/html; charset=None\">\n",
       "  <style type=\"text/css\">\n",
       "/*\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2021 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "*/\n",
       "pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "body .hll { background-color: #ffffcc }\n",
       "body { background: #ffffff; }\n",
       "body .c { color: #0000FF } /* Comment */\n",
       "body .k { color: #ff0000; font-weight: bold } /* Keyword */\n",
       "body .n { color: #008000 } /* Name */\n",
       "body .ch { color: #0000FF } /* Comment.Hashbang */\n",
       "body .cm { color: #0000FF } /* Comment.Multiline */\n",
       "body .cp { color: #0000FF } /* Comment.Preproc */\n",
       "body .cpf { color: #0000FF } /* Comment.PreprocFile */\n",
       "body .c1 { color: #0000FF } /* Comment.Single */\n",
       "body .cs { color: #0000FF } /* Comment.Special */\n",
       "body .kc { color: #ff0000; font-weight: bold } /* Keyword.Constant */\n",
       "body .kd { color: #ff0000; font-weight: bold } /* Keyword.Declaration */\n",
       "body .kn { color: #ff0000; font-weight: bold } /* Keyword.Namespace */\n",
       "body .kp { color: #ff0000; font-weight: bold } /* Keyword.Pseudo */\n",
       "body .kr { color: #ff0000; font-weight: bold } /* Keyword.Reserved */\n",
       "body .kt { color: #ff0000; font-weight: bold } /* Keyword.Type */\n",
       "body .s { color: #111111 } /* Literal.String */\n",
       "body .na { color: #008000 } /* Name.Attribute */\n",
       "body .nb { color: #008000 } /* Name.Builtin */\n",
       "body .nc { color: #008000 } /* Name.Class */\n",
       "body .no { color: #008000 } /* Name.Constant */\n",
       "body .nd { color: #008000 } /* Name.Decorator */\n",
       "body .ni { color: #008000 } /* Name.Entity */\n",
       "body .ne { color: #008000 } /* Name.Exception */\n",
       "body .nf { color: #008000 } /* Name.Function */\n",
       "body .nl { color: #008000 } /* Name.Label */\n",
       "body .nn { color: #008000 } /* Name.Namespace */\n",
       "body .nx { color: #008000 } /* Name.Other */\n",
       "body .py { color: #008000 } /* Name.Property */\n",
       "body .nt { color: #008000 } /* Name.Tag */\n",
       "body .nv { color: #008000 } /* Name.Variable */\n",
       "body .sa { color: #111111 } /* Literal.String.Affix */\n",
       "body .sb { color: #111111 } /* Literal.String.Backtick */\n",
       "body .sc { color: #111111 } /* Literal.String.Char */\n",
       "body .dl { color: #111111 } /* Literal.String.Delimiter */\n",
       "body .sd { color: #111111 } /* Literal.String.Doc */\n",
       "body .s2 { color: #111111 } /* Literal.String.Double */\n",
       "body .se { color: #111111 } /* Literal.String.Escape */\n",
       "body .sh { color: #111111 } /* Literal.String.Heredoc */\n",
       "body .si { color: #111111 } /* Literal.String.Interpol */\n",
       "body .sx { color: #111111 } /* Literal.String.Other */\n",
       "body .sr { color: #111111 } /* Literal.String.Regex */\n",
       "body .s1 { color: #111111 } /* Literal.String.Single */\n",
       "body .ss { color: #111111 } /* Literal.String.Symbol */\n",
       "body .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       "body .fm { color: #008000 } /* Name.Function.Magic */\n",
       "body .vc { color: #008000 } /* Name.Variable.Class */\n",
       "body .vg { color: #008000 } /* Name.Variable.Global */\n",
       "body .vi { color: #008000 } /* Name.Variable.Instance */\n",
       "body .vm { color: #008000 } /* Name.Variable.Magic */\n",
       "\n",
       "  </style>\n",
       "</head>\n",
       "<body>\n",
       "<h2></h2>\n",
       "\n",
       "<div class=\"highlight\"><pre><span></span><span class=\"s\">67                                                         The SAS System                       Saturday, April  3, 2021 09:00:00 PM</span><br><br><span class=\"s\">377        ods listing close;ods html5 (id=saspy_internal) file=_tomods1 options(bitmap_mode=&#39;inline&#39;) device=svg style=HTMLBlue;</span><br><span class=\"s\">377      ! ods graphics on / outputfmt=png;</span><br><span class=\"s\">378        </span><br><span class=\"s\">379        * To find P(T_19 ge 2.71);</span><br><span class=\"s\">380        data new;</span><br><span class=\"s\">381          t=1-CDF(&#39;T&#39;, 2.71, 19);</span><br><span class=\"s\">382        put t=;</span><br><span class=\"s\">383        RUN;</span><br><span class=\"err\">t=0.0069431549</span><br><span class=\"s\">384        </span><br><span class=\"s\">385        * To find 1-P(Z \\le 1.2344);</span><br><span class=\"s\">386        data new;</span><br><span class=\"s\">387          p=1-CDF(&#39;NORMAL&#39;, 1.2344);</span><br><span class=\"s\">388        put p=;</span><br><span class=\"s\">389        RUN;</span><br><span class=\"err\">p=0.1085269445</span><br><span class=\"s\">390        </span><br><span class=\"s\">391        </span><br><span class=\"s\">392        </span><br><span class=\"s\">393        ods html5 (id=saspy_internal) close;ods listing;</span><br><span class=\"s\">394        </span><br><br><span class=\"s\">68                                                         The SAS System                       Saturday, April  3, 2021 09:00:00 PM</span><br><br><span class=\"s\">395        </span><br></pre></div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%SAS my_session\n",
    "* To find P(T_19 ge 2.71);\n",
    "data new; \n",
    "  t=1-CDF('T', 2.71, 19);\n",
    "put t=;\n",
    "RUN;\n",
    "\n",
    "* To find 1-P(Z \\le 1.2344);\n",
    "data new; \n",
    "  p=1-CDF('NORMAL', 1.2344);\n",
    "put p=;\n",
    "RUN;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significance test and confidence interval example\n",
    "There are two methods of **inferential statistics**. \n",
    "\n",
    "1. Inference about _interval_ estimation by means of **confidence intervals**.\n",
    "2. Inference about point estimations using **significance tests**. \n",
    "\n",
    "**Example**\n",
    "\n",
    "Suppose you have a sample of $n=500$ divers with a mean diving time of $\\bar{x}=36$ hours and a standard deviation of $S=8$ hours. The sample distribution of the variable \"hours of diving experience\" is approximately normal.\n",
    "\n",
    "Based on this information, you want to draw inferences about the population parameter, $\\mu$. \n",
    "\n",
    "Here,\n",
    "$$\n",
    "H_{0}: \\mu = 35 \\\\\n",
    "H_{A}: \\mu \\neq 35\n",
    "$$\n",
    "\n",
    "Our assumptions are met - our analysis is based on a simple random sample and we're dealing with a large sample. This large sample is approximately normally distributed. Our test statistic is\n",
    "$$\n",
    "t = \\frac{\\bar{x} - \\mu}{SE} \\\\\n",
    "\\text{where } SE = \\frac{S}{\\sqrt{n}} \\\\\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "$$\n",
    "t = \\frac{36-35}{\\frac{8}{\\sqrt{500}}} \\approx 2.80\n",
    "$$\n",
    "\n",
    "Our sampling distribution looks like:\n",
    "<center><img src=\"ex_sampling_distribution.png\" style=\"width:400px\"/></center>\n",
    "\n",
    "\n",
    "To construct a **95% confidence interval**, we use the following:\n",
    "\n",
    "$$\n",
    "\\bar{x} \\pm t_{95\\%}(SE) \\\\\n",
    "\\text{where } SE = \\frac{\\text{standard deviation}}{\\sqrt{n}}\n",
    "$$\n",
    "\n",
    "The relevant t-score is 1.984, so we have \n",
    "$$\n",
    "36 \\pm 1.984(\\frac{8}{\\sqrt{500}})\n",
    "$$\n",
    "\n",
    "Thus, the 95% confidence interval is (35.29, 36.71). We can be confident that, with repeated sampling, this interval would contain the actual population mean 95% of the time. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Tip:</b> If the p-value in a two-tailed significance test is $\\le 0.05$, the 95% confidence interval does NOT contain the $H_{0}$ value. Conversely, if the p-value in a two-tailed significance test is $\\gt 0.05$, then the 95% confidence interval WILL contain the $H_{0}$ value. \n",
    "</div>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common hypotheses tests for a single mean\n",
    "RR = rejection region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"common_ht_single_mean.png\" style=\"width:800px\"/></center>\n",
    "<center><img src=\"common_ht_single_mean_2.png\" style=\"width:800px\"/></center>\n",
    "<center><img src=\"common_CI_differences_of_means.png\" style=\"width:800px\"/></center>\n",
    "<center><img src=\"common_CI_differences_of_means_2.png\" style=\"width:800px\"/></center>\n",
    "<center><img src=\"common_CI_variances.png\" style=\"width:800px\"/></center>\n",
    "<center><img src=\"common_CI_variances_2.png\" style=\"width:800px\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the goal of a hypothesis test?**\n",
    "\n",
    "For a hypothesis test, we make assumptions about some aspect of our population (usually a parameter) and see if our data refutes that assumption.  This gives us a way to make decisions about our population using our data.\n",
    "\n",
    "**What do we use a p-value for?**\n",
    "\n",
    "p-values provide a measure of evidence against the null hypothesis. We look at p-values to determine our conclusion. \n",
    "\n",
    "**What is meant by a ‘z-test’?**\n",
    "\n",
    "A hypothesis test that uses the normal distribution for its test statistic.\n",
    "\n",
    "**When is a ‘z-test’ reasonable when doing a hypothesis test for a population mean?**\n",
    "\n",
    "When you have a random sample of size n from a normally distributed population where you know the population variance/standard deviation.\n",
    "When you have a large random sample from (almost) any population.\n",
    "\n",
    "Note that when you have a ‘large’ random sample from (almost) any population, a ‘t-test’ is basically the same as a ‘z-test’. This means you might just use a t-test anyway!\n",
    "\n",
    "**We can use a p-value to make a decision about H0.  We reject H0 when our p-value is less than alpha.  When we use this rule to make a conclusion about H0, how does this control alpha?**\n",
    "\n",
    "Alpha = Probability we reject H0 when H0 is true = P(Reject H0|H0)\n",
    "P-value = P(result as or more extreme|H0)\n",
    "We calculate the p-value assuming H0 is true. We use the rule that we only reject when the p-value is less than alpha. \n",
    "Assuming H0 is true, we should only see samples that give us a p-value less than alpha in $100*alpha\\%$ of our experiments.  Again since H0 is assumed true when finding the p-value, we will only falsely reject H0 with probability alpha.\n",
    "\n",
    "**What do we mean by the term \"controlling alpha\"?**\n",
    "\n",
    "We mean keeping this probability (P(reject H0|H0)) fixed at a small value (often 0.05).  Since type I errors are usually considered worse, we want to ‘control’ the probability of making a type I error – i.e. we want to control alpha by keeping it small.\n",
    "\n",
    "**What is a p-value?**\n",
    "\n",
    "A p-value is the probability of seeing a result as or more extreme than what was observed, assuming the null hypothesis is true.\n",
    "\n",
    "**How does a rejection region control $\\alpha$?**\n",
    "\n",
    "The rejection region is set up assuming the null hypothesis is true. In fact, it is set up so that, assuming the null is true, we should only observe our test statistic in that region with probability alpha.  That means we will only reject H0 when H0 is true with probability alpha.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type I and Type II errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Power**: the probability of rejecting the null hypothesis given that it is false. We want low $\\alpha$, low $\\beta$, high power. Power is $1-\\beta$. Larger samples increase power. \n",
    "\n",
    "Power is important because, before we conduct a study, it can help us determine how many participants we need. After we've conducted the study, it can help us make sense of results that are not statistically significant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you're a diver interested in whale sharks and you'd like to know what the average length of these gigantic animals. Also suppose you have spent years and years in different parts of the world to study these creatures.\n",
    "\n",
    "Over the years you have encountered and measured 258 whale sharks. Because you have measured whale sharks all over the world, we assume from now that these 258 whale sharks can be understood as a _simple random sample_.\n",
    "\n",
    "It turns out that the mean length equals 8.3 meters. The sample standard deviation is 3.4 meters. It also turns out that the distribution of whale shark length is approximately normal.\n",
    "\n",
    "$$\n",
    "n = 258 \\text{ whale sharks}\\\\\n",
    "\\bar{x} = 8.3 \\text{ meters} \\\\\n",
    "\\text{sample standard deviation} = s = 3.4 \\text{ meters}\n",
    "$$\n",
    "\n",
    "We will test three alternative hypotheses against the null hypothesis that the mean whale shark length in the population is 8 meters.\n",
    "\n",
    "$$\n",
    "H_{0}: \\mu = 8 \\text{ meters} \\\\\n",
    "--- \\\\\n",
    "H_{A_{1}}: \\mu \\neq 8 \\text{ meters} \\\\\n",
    "H_{A_{2}}: \\mu \\gt 8 \\text{ meters} \\\\\n",
    "H_{A_{3}}: \\mu \\lt 8 \\text{ meters} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all cases, **$\\alpha = 0.10$**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Check our assumptions.\n",
    "* Randomization\n",
    "* Population distribution of whale shark lengths: \"approximately normal\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compute the test statistic.\n",
    "\n",
    "For $H_{A_{1}}: \\mu \\neq 8$:\n",
    "\n",
    "$$\n",
    "\\text{test statistic} = t = \\frac{\\bar{x} - \\mu}{SE} \\\\\n",
    "SE = \\frac{s}{\\sqrt{n}} \\\\\n",
    "------------\\\\\n",
    "t = \\frac{8.3 - 8}{\\frac{3.4}{\\sqrt{258}}} \\approx 1.42\\\\\n",
    "$$\n",
    "\n",
    "For two-tailed $\\alpha=0.10$, $t_{90\\%} \\approx 1.66$.\n",
    "\n",
    "<center><img src=\"ex_1_two_tailed.png\" style=\"width:400px\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $H_{A_{2}}: \\mu \\gt 8$:\n",
    "\n",
    "$$\n",
    "\\text{test statistic} = t = \\frac{\\bar{x} - \\mu}{SE} \\\\\n",
    "SE = \\frac{s}{\\sqrt{n}} \\\\\n",
    "------------\\\\\n",
    "t = \\frac{8.3 - 8}{\\frac{3.4}{\\sqrt{258}}} \\approx 1.42\\\\\n",
    "$$\n",
    "\n",
    "Instead of a two-tailed test, we now do a one-tailed test. For $\\alpha=0.10$, one-tail $p \\approx 1.29$. Since $t=1.42$ is now within the rejection region, we DO reject the null hypothesis and conclude that the population mean is indeed larger than 8.\n",
    "\n",
    "<center><img src=\"ex_1_one_tailed.png\" style=\"width:400px\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $H_{A_{3}}: \\mu \\lt 8$:\n",
    "\n",
    "$$\n",
    "\\text{test statistic} = t = \\frac{\\bar{x} - \\mu}{SE} \\\\\n",
    "SE = \\frac{s}{\\sqrt{n}} \\\\\n",
    "------------\\\\\n",
    "t = \\frac{8.3 - 8}{\\frac{3.4}{\\sqrt{258}}} \\approx 1.42\\\\\n",
    "$$\n",
    "\n",
    "Instead of a two-tailed test, we now do a one-tailed test. For $\\alpha=0.10$, one-tail $p \\approx 1.29$, so our critical value is -1.29. Since $t=1.42$ is not within the rejection region, we do not reject the null hypothesis and we cannot conclude that the population mean is smaller than 8.\n",
    "\n",
    "<center><img src=\"ex_1_one_tailed_left.png\" style=\"width:400px\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with hypotheses tests in SAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: z-test for a **proportion**\n",
    "Consider a data set collected on Europeans. The goal is to test if the proportion of Europeans with blue eyes differs from 0.5. That is, the null hypothesis is that half of Europeans have blue eyes and the alternative is \"not half\" (more or less than a half). \n",
    "\n",
    "$$\n",
    "H_{0}: 0.5 \\\\\n",
    "H_{A}: p \\neq 0.5\n",
    "$$\n",
    " \n",
    "**Question**: What SAS procedure that we’ve studied allows us to do a hypothesis test for a proportion?\n",
    "\n",
    "**Answer**: `PROC FREQ`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%SAS my_session\n",
    "* z-test for a proportion;\n",
    "PROC FREQ DATA = Color;\n",
    "  TABLES Eyes / BINOMIAL(Wald);\n",
    "RUN;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"sas_example_z_test_for_proportion.png\" style=\"width:800px\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're testing if the proportion is _less than_ **or** _great than_ 0.5, we want a two-sided test. This requires us to multiply the p-value by 2. The area to the left of -1.7321 is around 4.16% or 0.0416, so we double this to account for the area on the other side. This gives us a p-value of 0.0833.\n",
    "\n",
    "Thus, using a significance level of $\\alpha=0.05$, we fail to reject the null hypothesis that the proportion of blue-eyed Europeans is 0.5 ($0.0833 \\gt 0.05$). We don't have enough evidence to that that the proportion differs from 0.5.\n",
    "\n",
    "If we had been testing _only_ $H_{A}: p \\neq 0.5$, _then_ we would have been able to reject the null hypothesis.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: t-test for a **mean**\n",
    "Consider a data set with court cases and their lengths. The population is all court cases in some district, the sample is $n=20$ court cases on variable days, and let's make an inference about $\\mu$, the average length of all court cases.\n",
    "\n",
    "$$\n",
    "H_{0}: \\mu = 60 days \\\\\n",
    "H_{A}: \\mu \\gt 60 days\n",
    "$$\n",
    "\n",
    "**Question**: What SAS procedure that we’ve studied allows us to do a one-sample t-test for a mean?\n",
    "\n",
    "**Answer**: `PROC TTEST`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a small data set, so let's check our assumptions to make sure the data is _normally distributed_.\n",
    "\n",
    "<center><img src=\"sas_example_t_test_for_mean_assumptions.png\" style=\"width:800px\"/></center>\n",
    "\n",
    "We have linearity (roughly speaking), so let's go to the next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%SAS my_session\n",
    "/* t-test for a mean. H0 gives SAS the null value, 60. We also have to\n",
    "tell SAS if we want a one-sided test or a two-sided test.\n",
    "Here, we want a one-sided test, specifically an upper test (great than),\n",
    "so our SIDES = option is \"U\". If SIDES = L, then we are testing for \"less than\" 60 days */\n",
    "PROC TTEST DATA = cases ALPHA=0.01 H0 = 60 SIDES = U PLOTS = all;\n",
    "  VAR days;\n",
    "RUN;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"sas_example_t_test_for_mean.png\" style=\"width:800px\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, our cases were taking 89.85 days. We have a standard error of 4.2811 around that 89.85 days. Our test statistic, the t-value, is 6.97, comes from a t-distribution with 19 degrees of freedom. The p-value is less than 0.0001 (very rare).  \n",
    "\n",
    "We have enough evidence to conclude that the mean number of days for a court case is greater than 60. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: difference of means\n",
    "Consider the Cookie Cats game. The population is all possible players (now and in the future). The sample is approximately 90,000 players that installed the game during the study. Players were randomly assigned to have a gate at level 30 or level 40 when they installed. \n",
    "\n",
    "Make an inference about the _difference_ in gates between average number of games played, $\\mu_{Diff} = \\mu_{30} - \\mu_{40}$.\n",
    "\n",
    "$$\n",
    "H_{0}: \\mu_{Diff} = 0 \\\\\n",
    "H_{A}: \\mu_{Diff} \\neq 0\n",
    "$$\n",
    "\n",
    "**Question**: What SAS procedure that we’ve studied allows us to do a one-sample t-test for a mean?\n",
    "\n",
    "**Answer**: `PROC TTEST`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That the data in this set is NOT normally distributed. \n",
    "<center><img src=\"cookie_cats_qq.png\" style=\"width:800px\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can transform this data using the _log_ of the sum of the game rounds to make normality much more reasonable. \n",
    "<center><img src=\"cookie_cats_log_qq.png\" style=\"width:800px\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%SAS my_session\n",
    "* Difference of means;\n",
    "PROC TTEST DATA = cats PLOTS = all;\n",
    "  CLASS version;\n",
    "  VAR log_sum_gamerounds;\n",
    "RUN;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We typically use the unequal variance assumption and, therefore, the Satterhwaite approximation. \n",
    "<center><img src=\"sas_example_difference_of_means.png\" style=\"width:800px\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $\\alpha=0.05$, we would reject if $p-value \\lt 0.05$. Our p-values are 0.9 (very high). We do not have enough evidence to claim that gates 30 and 40 are different. We fail to reject the null hypothesis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: paired t-test\n",
    "Jocko's garage seems to be giving out really high estimates for insurance claims. To investigate insurance fraud, insurance adjusters take 10 damaged cars and take each one to both Jocko's and a repair shop they trust, Jami's repair shop. They then get estimates from each repair shop (in the end, 2 for each car).\n",
    "- The cars are the unit of measurement\n",
    "- **Measured twice so paired data!**\n",
    "- Measurements are clearly related and **cannot be treated as independent**\n",
    "\n",
    "We're **NOT** looking at the average of Jocko and the average of Jami and taking the different. We're taking the difference first! We're then looking at the average of these differences to see if that average is equal to 0.\n",
    "\n",
    "$$\n",
    "H_{0}: \\mu_{Diff} = 0 \\\\\n",
    "H_{A}: \\mu_{Diff} \\gt 0 \\text{ (i.e. Jocko is keeping some money for himself)}\n",
    "$$\n",
    "\n",
    "**Question**: What SAS procedure that we’ve studied allows us to do a one-sample t-test for a mean?\n",
    "\n",
    "**Answer**: `PROC TTEST`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%SAS my_session\n",
    "* Difference of means;\n",
    "PROC TTEST DATA = insurance PLOTS = all;\n",
    "  PAIRED Jocko*Jami\n",
    "RUN;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"sas_example_paired_t_test.png\" style=\"width:800px\"/></center>\n",
    "\n",
    "The probability 0.0014 is less than $\\alpha=0.05$ and we can reject the null hypothesis. Jocko needs to be investigated further. His estimates are, on average, $160 higher than Jami's. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing confidence intervals and hypotheses tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seem to get the same conclusions when we perform either test. This is because the two methods are related!\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Tip:</b> \n",
    "<br>If the null value (e.g. $\\mu_{0}$) is contained in a $100(1- \\alpha)$% confidence interval for $\\mu$, then we fail to reject $H_{0}$ at level $\\alpha$.  \n",
    "    \n",
    "<br>If the null value is NOT contained in a $100(1- \\alpha)$% confidence interval for $\\mu$, then we reject $H_{0}$ at level $\\alpha$. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"ci_vs_ht.png\" style=\"width:800px\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"type_I_type_II_errors.png\" style=\"width:400px\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Power = 1 - \\beta = P(\\text{Reject } H_{0}|H_{A} \\text{ true})\n",
    "$$\n",
    "\n",
    "High power means low type II error. We often use 80% power. We can manipulate power by manipulating sample sizes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "Consider data on length of court cases\n",
    "- Population = all court cases (in some district)\n",
    "- RV is Y = length of a court case\n",
    "- Assumed to be normally distributed\n",
    "- We want to test (using $\\alpha=0.05$)\n",
    "\n",
    "$$\n",
    "H_{0}: \\mu = 60 \\text{ days} \\\\\n",
    "H_{A}: \\mu \\gt 60 \\text{ days}\n",
    "$$\n",
    "\n",
    "We want to have a power of 80% to detect a true mean of 65 days. What sample size do we need?\n",
    "\n",
    "We must have a value to use for $\\sigma$. Suppose that, from past court case data, we can estimate $\\sigma$ to be 16 days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "n = \\frac{(z_{\\alpha}+z_{\\beta})^2\\sigma^2}{(\\mu_{0}-\\mu_{A})^2} \\\\\n",
    "= \\frac{(1.645+0.842)^2 16^2}{(60-65)^2} \\approx 63.3\n",
    "$$\n",
    "\n",
    "We need at least 63.3 observations. So, if we use 64 cases and the true mean is actually 65, we will reject the null hypothesis in 80% of all possible experiments. We would have a type II error rate of 20%.\n",
    "\n",
    "Note that $z_{\\alpha}$ is the value from the standard normal distribution with $\\alpha$ area to the right of it (i.e. the $1-\\alpha$ quantile of the standard normal).\n",
    "\n",
    "Similarly, $z_{\\beta}$ is the value from the standard normal distribution with $\\beta$ area to the right of it (i.e. the $1-\\beta$ quantile of the standard normal).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
