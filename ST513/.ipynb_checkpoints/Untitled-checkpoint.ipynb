{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SAS Config named: oda\n",
      "SAS Connection established. Subprocess id is 5848\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import saspy\n",
    "my_session = saspy.SASsession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation and Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often report an estimate and its **standard error (SE)**. Standard error is the standard deviation of a _statistic_ (rather than of the population). For a random sample, the standard error of the sample mean (SEM) is \n",
    "\n",
    "$$\n",
    "SE(\\bar{Y}) = \\frac{\\sigma}{\\sqrt{n}} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we do not know the standard deviation of the _population_ ($\\sigma$), we can estimate the standard error of the sample mean as\n",
    "\n",
    "$$\n",
    "\\hat{SE}(\\bar{Y}) = \\frac{s}{\\sqrt{n}} \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are observing the wingspan of 25 butterflies with a sample mean of 3.53 and a sample standard deviation of 0.17, then\n",
    "\n",
    "$$\n",
    "SEM = \\frac{0.17}{\\sqrt{25}} \\approx 0.034 \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** What do we mean by the term \"estimation\"?\n",
    "\n",
    "**A.** Attempting to use data to give a value (or range of values) for a parameter.\n",
    "\n",
    "**Q.** Why do we need to report more than a point estimate when using data to describe a parameter?\n",
    "\n",
    "**A.** A point estimate alone does not given an idea of variability. We usually provide a standard error or a confidence interval as well.\n",
    "\n",
    "**Q.** What does the standard error of a statistic measure?\n",
    "\n",
    "**A.** Variability (standard deviation) in the statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequentist estimation via method of moments and maximum likelihood (the math behind estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A point _estimate_ (e.g. the actual number $\\bar{y}$ or $s$) is an observed value of a point _estimator_ (a random variable, such as $\\bar{Y}$ or $S$, that we will use to make an inference). $\\bar{Y}$, which we often observe in a normal distribution, is a common estimator of the population mean, $\\mu$. Estimators have several key properties:\n",
    "* If we take many different samples and look at the estimator across all of those samples, the average of the estimators should give the true population value (an \"unbiased\" estimator)\n",
    "* The estimator should have as little variation as possible from dataset to dataset (a small standard error, SE)\n",
    "* The bigger the sample, the closer the estimator is to the true population value (\"consistent\" estimator)\n",
    "\n",
    "A point estimator is a random quantity that has yet to be observed.  For instance, the idea of taking a sample mean of the amount of debt of 100 randomly selected people.  The quantity isn’t observed yet and is random – it has a distribution, mean, standard error, etc.  We usually denote estimators with uppercase values.\n",
    "\n",
    "A point estimate is a fixed quantity – the observed value of the estimator.  For instance, if we observed the 100 randomly selected people and their sample mean amount of debt was 175 thousand.  This quantity is fixed or known.  We usually denote estimates with lowercase values.\n",
    "\n",
    "**Q.** What do we mean by the term \"consistent estimator\"?\n",
    "\n",
    "**A.** An estimator that is observed arbitrarily close to the parameter as the sample size increases is a consistent estimator.  This just implies that our estimator eventually is observed right at the truth.\n",
    "\n",
    "Note: If the bias of the estimator is 0 or goes away as the sample size grows and the standard error shrinks toward 0 as the sample size grows, the estimator will be consistent! This should make some sense, bias tells us where the estimator is observed on average and standard error tells us how variable our estimator is. If on average we take on the true value and our variation decreases towards 0, we must be observing closer and closer to the truth!\n",
    "\n",
    "**Q.** What is the basic idea of the Method of Moments estimation procedure?\n",
    "\n",
    "**A.** We take sample moments (such as $\\bar{Y}$) and set them equal to population moments (such as E(Y)) and solve the equations of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two common methods for creating an estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method of Moments (MOM)**\n",
    "\n",
    "If we have a random sample, MOM uses the sample average and the population averages to create estimators. On the upside, MOM makes the finding of estimators easy and MOM estimators are consistent; on the downside, the distribution of the created estimator can be difficult to determine and gamma MOM estimators are not unbiased (taking more samples does not bring us closer to the right answer). \n",
    "\n",
    "_Gamma Distribution Example_: We have a gamma distribution $Y\\sim Gamma(\\alpha,\\beta)$ where $Y$ is the time spent reading a news article. For gamma distributions,\n",
    "\n",
    "$$\n",
    "\\mu = E(Y) = \\frac{\\alpha}{\\beta}\\\\\n",
    "\\sigma^2 = Var(Y) = \\frac{\\alpha}{\\beta^2} \\\\\n",
    "E(Y^2) = \\frac{\\alpha(\\alpha+1)}{\\beta^2}\n",
    "$$\n",
    "\n",
    "where $E(Y)$ and $E(Y^2)$ are the population (raw) _moments_. We can observe a random sample of times spent reading a news article (a random sample of $Y$s). The Law of Large Numbers states that, for large sample sizes, the sample averages should be very close to the population averages. That is,\n",
    "\n",
    "$$\n",
    "\\hat{\\alpha} = \\frac{\\bar{Y}^2}{\\frac{\\sum_{i=1}^{n} Y_{i}^2}{n} - \\bar{Y}^2} \\approx\\frac{\\bar{y}^2}{\\frac{\\sum_{i=1}^{n} y_{i}^2}{n} - \\bar{y}^2} \\\\ \\text{ and }\\\\\n",
    "\\hat{\\beta} = \\frac{\\bar{Y}}{\\frac{\\sum_{i=1}^{n} Y_{i}^2}{n} - \\bar{Y}^2} \\approx \\frac{\\bar{y}}{\\frac{\\sum_{i=1}^{n} y_{i}^2}{n} - \\bar{y}^2}\n",
    "$$\n",
    "\n",
    "_Binomial/Bernoulli MOM example_: Our population is all of the customers at a bank and each customer follows a Bernoulli distribution. Our parameter $p$ is the proportion of customers willing to open an additional account. We observe 40 random customers. We can define $X_{i}=1$ if a customer $i$ opens an account, where $X_{i} \\sim ^{iid} Ber(p)$. We can also define $Y$ as the number of customers opening an account, where \n",
    "\n",
    "$$\n",
    "Y = \\sum_{i=1}^{n} X_{i}\\sim Bin(n,p)\n",
    "$$\n",
    "\n",
    "What is the MOM estimator of $p$?\n",
    "\n",
    "MOM tells us to set the sample average to the population average.\n",
    "\n",
    "$$\n",
    "X \\sim Ber(p) \\\\\n",
    "E(X) = p \\\\\n",
    "Var(X) = p(1-p)\n",
    "$$\n",
    "\n",
    "Here, $\\bar{X} \\approx p$ and \n",
    "\n",
    "$$\n",
    "\\text{Sample proportion} = \\hat{p} = \\frac{Y}{n}\n",
    "$$\n",
    "\n",
    "This estimator is unbiased (on average, it gives us $p$) and consistent and\n",
    "\n",
    "$$\n",
    "\\text{Standard error} = SE(\\hat{p}) = \\sqrt{\\frac{p(1-p)}{n}}\n",
    "$$\n",
    "\n",
    "Moreover, $\\hat{p}$ can be approximated using a normal distribution so that\n",
    "\n",
    "$$\n",
    "\\hat{p} \\sim N(p,\\sqrt{\\frac{p(1-p)}{n}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Maximum Likelihood (ML)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML uses the assumed curve to find the \"most likely\" values of the parameters to produce the data we see. Mathematically, they are more difficult, but they are generally consistent and the distribution of an MLE can often be approximated with a normal curve.\n",
    "\n",
    "_Exponential distribution example_: The exponential distribution is a special case of the gamma with $\\alpha = 1$. All we need to estimate is the $\\beta$. If $Y$ is the time spent reading a news article, then \n",
    "\n",
    "$$\n",
    "Y \\sim Gamma(1, \\beta) \\sim (Exp \\beta) \\\\\n",
    "f(y) = \\beta e^{-\\beta y}, y \\gt 0 \\\\\n",
    "\\mu = E(Y) = \\frac{1}{\\beta} \\\\\n",
    "\\sigma^2 = Var(Y) = \\frac{1}{\\beta^2}\n",
    "$$\n",
    "\n",
    "The PDF tells us \"give me a $\\beta$ and I will give you an observed value $y$\". The likelihood reverses that and tells us \"give me an observed value $y$ and I will give you a $\\beta$\". \n",
    "\n",
    "$$\n",
    "f(y | \\beta) = \\beta e^{-\\beta y}, y \\gt 0, \\beta \\gt 0 \\\\\n",
    "L(\\beta | y) = \\beta e^{-\\beta y}, y \\gt 0, \\beta \\gt 0\n",
    "$$\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Tip:</b> For generic $y$ value, the MLE for the exponential distribution is \n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = \\frac{1}{y}\n",
    "$$\n",
    "\n",
    "That is, the most likely value of $\\beta$ to produce $y$ is 1/$y$. For a random sample of $Y$s, $\\hat{\\beta}_{MLE} = \\frac{1}{\\bar{Y}}$\n",
    "</div>\n",
    "\n",
    "For a random sample of $Y$s, \n",
    "\n",
    "$$\n",
    "Y_{i} \\sim ^{iid} Exp(\\beta), i=1,...,n\n",
    "$$\n",
    "\n",
    "We can find the joint distribution as follows\n",
    "\n",
    "$$\n",
    "A \\text{ independent } B = P(A \\cap B) = P(A)P(B)\\\\\n",
    "f(y_1, y_2,...,y_n) = f(y_1)f(y_2)...f(y_n) \\\\\n",
    "=\\beta e^{-\\beta y_1}\\beta e^{-\\beta y_2}...\\beta e^{-\\beta y_n} \\\\\n",
    "= \\beta^n e^{-\\beta \\sum_{i=1}^{n} y_{i}}\n",
    "$$\n",
    "\n",
    "**Q.** What is the difference between a joint distribution (say $f(y_1,...,y_n|parameters)$) and a likelihood (say $L(parameters|y_1,...,y_n)$)?\n",
    "\n",
    "**A.** With a joint distribution, we consider the parameters to be known and the y’s can be varied (for instance in finding $P(Y_{1} \\gt 10, Y_{2} \\gt 5)$. With a likelihood, we assume the data is known and that the parameters can be varied. This allows us to maximize the function with respect to the parameter value(s). The value of the parameter that corresponds to this maximum is called the maximum likelihood estimator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common ML and MOM comparison\n",
    "\n",
    "<center><img src=\"mom_ml_estimators.png\" style=\"width:600px\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
