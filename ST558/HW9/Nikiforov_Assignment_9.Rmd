---
title: "Assignment 9"
author: "Maksim Nikiforov"
date: "10/19/2021"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
```

## Part 1: kNN

### 1. 

_Read in the `heart.csv` data file._

```{r , echo=TRUE, eval=TRUE}
# Read in space-separated data using "read_csv()" function.
# Remove column ST_Slope, convert HeartDisease to a factor
heartData <- read.csv(file = "./heart.csv") %>% select(-ST_Slope)
heartData
``` 

### 2. 

_Create dummy columns corresponding to the values of `Sex`, `ChestPainType`, and `RestingECG` for use in our kNN fit._

```{r , echo=TRUE, eval=TRUE}
# Read in space-separated data using "read_csv()" function.
head(model.matrix(HeartDisease ~ ., data = heartData))
``` 

```{r , echo=TRUE, eval=TRUE}
# Read in space-separated data using "read_csv()" function.
dummies <- dummyVars( ~ ., data = heartData)
newHeartData <- predict(dummies, newdata = heartData)
newHeartData <- as_data_frame(newHeartData)
newHeartData$HeartDisease <- as.factor(newHeartData$HeartDisease)
newHeartData
``` 

### 3.

_Now split the data set you’ve created into a training and testing set. Use `p = 0.8`._

Here, we see that a medium-colored crab with both spines in "good" condition had as many as 14 satellites. Crabs with one spine worn or broken did not exceed 6 satellites.  

```{r , echo=TRUE, eval=TRUE}
# p = 0.8 places 80% of observations in the training set
set.seed(50)
heartIndex <- createDataPartition(newHeartData$HeartDisease, p = 0.8, list = FALSE)
heartTrain <- newHeartData[heartIndex, ]
heartTest <- newHeartData[-heartIndex, ]
```

### 4.
_Finally, train the kNN model. Use repeated 10 fold cross-validation, with the number of repeats being 3. You should also preprocess the data by centering and scaling. Lastly, set the `tuneGrid` so that you are considering values of k of 1, 2, 3, …, 40._

```{r , echo=TRUE, eval=TRUE}
# 
kNNFit <- train(HeartDisease ~ ., data = heartTrain,
               method = "knn",
               preProcess = c("center", "scale"),
               trControl = trainControl(method = "repeatedcv", 
                                        number = 10, repeats = 3),  
               tuneGrid = data.frame(k = 1:40)) 
```

### 5. 

_Check how well your model does on the test set using the confusionMatrix() function._

```{r , echo=TRUE, eval=TRUE}
# See confusion matrix on test set (correct/incorrect predictions)
confusionMatrix(data = heartTest$HeartDisease, reference = predict(kNNFit, newdata = heartTest))
```

## Part 2: Ensemble

We’ll look at predicting the same heart disease variable in this section as well, just instead of using kNN we’ll use the following methods:

### 1.

_A classification tree (use `method = rpart`: tuning parameter is `cp`, use values 0, 0.001, 0.002, …, 0.1)_

```{r , echo=TRUE, eval=TRUE}
# Create tuning parameter
rpartParam <- seq(from = 0, to = 0.1, by = 0.001)

# Classification tree fit
rpartFit <- train(HeartDisease ~ ., data = heartTrain,
               method = "rpart",
               preProcess = c("center", "scale"),
               trControl = trainControl(method = "repeatedcv", 
                                        number = 10, repeats = 3),  
               tuneGrid = data.frame(cp = rpartParam)) 

# See confusion matrix on test set (correct/incorrect predictions)
confusionMatrix(data = heartTest$HeartDisease, reference = predict(rpartFit, newdata = heartTest))
```

### 2.

_A bagged tree (use `method = treebag`: no tuning parameter)._

```{r , echo=TRUE, eval=TRUE}
# Bagged tree fit
baggedTreeFit <- train(HeartDisease ~ ., data = heartTrain,
               method = "treebag",
               preProcess = c("center", "scale"),
               trControl = trainControl(method = "repeatedcv", 
                                        number = 10, repeats = 3)) 

# See confusion matrix on test set (correct/incorrect predictions)
confusionMatrix(data = heartTest$HeartDisease, 
                reference = predict(baggedTreeFit, newdata = heartTest))
```

### 3.

_A random forest (use `method = rf`: tuning parameter is `mtry`, use vales of 1, 2, …, 15)._

```{r , echo=TRUE, eval=TRUE}
# Random forest fit
rfFit <- train(HeartDisease ~ ., data = heartTrain,
               method = "rf",
               preProcess = c("center", "scale"),
               trControl = trainControl(method = "repeatedcv", 
                                        number = 10, repeats = 3),  
               tuneGrid = data.frame(mtry = 1:15)) 

# See confusion matrix on test set (correct/incorrect predictions)
confusionMatrix(data = heartTest$HeartDisease, reference = predict(rfFit, newdata = heartTest))
```

### 4.

_A boosted tree (use `method = gbm`: tuning parameters are `n.trees`, `interaction.depth`, `shrinkage`, and `n.minobsinnode`, use all combinations of `n.trees` of 25, 50, 100, 150, and 200, `interaction.depth` of 1, 2, 3, 4, `shrinkage` = 0.1, and `n.minobsinnode` = 10; Hint: use `expand.grid()` to create your data frame for `tuneGrid`)._

```{r , echo=TRUE, eval=TRUE}
# Random forest fit
rfFit <- train(HeartDisease ~ ., data = heartTrain,
               method = "rf",
               preProcess = c("center", "scale"),
               trControl = trainControl(method = "repeatedcv", 
                                        number = 10, repeats = 3),  
               tuneGrid = data.frame(mtry = 1:15)) 

# See confusion matrix on test set (correct/incorrect predictions)
confusionMatrix(data = heartTest$HeartDisease, reference = predict(rfFit, newdata = heartTest))
```